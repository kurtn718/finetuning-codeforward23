{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1283849-9b84-4a0d-b304-c3022cb69adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda/envs/llm-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import gc\n",
    "\n",
    "from typing import Any, Callable\n",
    "\n",
    "import time\n",
    "\n",
    "from functools import wraps\n",
    "from inspect import (\n",
    "    BoundArguments,\n",
    "    signature,\n",
    ")\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import transformers\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, LlamaForCausalLM, MistralForCausalLM, AutoTokenizer, LlamaTokenizerFast, GenerationConfig, TextGenerationPipeline, BatchEncoding\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "\n",
    "from peft import PeftModel, PeftModelForCausalLM, PeftConfig, LoraConfig\n",
    "\n",
    "from ludwig.api import LudwigModel, TrainingResults\n",
    "\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805b3891-dfeb-4291-b4e9-a22ee05b7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: LudwigModel, df_test: pd.DataFrame) -> list[list[str]]:\n",
    "  return model.predict(df_test)[0][\"summary_response\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e18161-2034-47b3-ab30-427bfb20b39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/1.04M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 1.04M/1.04M [00:00<00:00, 4.96MB/s]\u001b[A\n",
      "Downloading data files:  33%|███▎      | 1/3 [00:00<00:00,  4.57it/s]\n",
      "Downloading data:   0%|          | 0.00/346k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 346k/346k [00:00<00:00, 2.30MB/s]\u001b[A\n",
      "Downloading data files:  67%|██████▋   | 2/3 [00:00<00:00,  5.40it/s]\n",
      "Downloading data:   0%|          | 0.00/344k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 344k/344k [00:00<00:00, 2.10MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00,  5.38it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 757.32it/s]\n",
      "Generating train split: 6000 examples [00:00, 97595.67 examples/s]\n",
      "Generating validation split: 2000 examples [00:00, 143223.63 examples/s]\n",
      "Generating test split: 2000 examples [00:00, 146349.52 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'correct_answer', 'scrambled', 'word'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'correct_answer', 'scrambled', 'word'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'correct_answer', 'scrambled', 'word'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrambled_dataset_dict: DatasetDict = datasets.load_dataset(\"kurtn718/scrambled_words_multiple_choice\")\n",
    "scrambled_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05365879-49a3-4a96-b7fe-edf9a62cdba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset: Dataset = scrambled_dataset_dict[\"train\"]\n",
    "test_dataset: Dataset = scrambled_dataset_dict[\"test\"]\n",
    "validation_dataset: Dataset = scrambled_dataset_dict[\"validation\"]\n",
    "\n",
    "df_train: pd.DataFrame = train_dataset.to_pandas()\n",
    "df_test: pd.DataFrame = test_dataset.to_pandas()\n",
    "df_validation: pd.DataFrame = validation_dataset.to_pandas()\n",
    "\n",
    "df_train = df_train.sample(n=700, random_state=200)\n",
    "df_test = df_test.sample(n=200, random_state=200)\n",
    "df_validation = df_validation.sample(n=100, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3746172b-a873-4af1-93ea-a8fff381dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_train.shape[0] == 700\n",
    "assert df_test.shape[0] == 200\n",
    "assert df_validation.shape[0] == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "762d34de-0846-4809-bac3-6da2813349f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"split\"] = np.zeros(df_train.shape[0])\n",
    "df_test[\"split\"] = np.ones(df_test.shape[0])\n",
    "df_validation[\"split\"] = np.full(df_validation.shape[0], 2)\n",
    "\n",
    "df_dataset = pd.concat([df_train, df_test, df_validation])\n",
    "\n",
    "df_dataset[\"split\"] = df_dataset[\"split\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f7d07e-4b9e-4ac7-b49e-0db90f334fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template: str = \"\"\"\n",
    "Answer this multiple choice question:\n",
    "\n",
    "### Question: {question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c51dbf7-602c-4c3b-a45c-bf2aa749335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_7b_sharded_base_model_name: str = \"alexsherstinsky/Mistral-7B-v0.1-sharded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feb0f568-75b1-49ad-84ca-102ed9c3ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config_base_model: BitsAndBytesConfig = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f5b15c3-1e4a-4a3d-b8c7-a66b6929341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 979/979 [00:00<00:00, 2.15MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 10.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 8.85MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 211kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 145/145 [00:00<00:00, 811kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_model_tokenizer: LlamaTokenizerFast = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=mistral_7b_sharded_base_model_name, trust_remote_code=True, padding_side=\"left\")\n",
    "print(base_model_tokenizer.eos_token)\n",
    "base_model_tokenizer.pad_token = base_model_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2a4d75-c0db-4b78-901d-c2d3847a0faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model: MistralForCausalLM = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=mistral_7b_sharded_base_model_name, device_map=\"auto\", torch_dtype=torch.float16, offload_folder=\"offload\", trust_remote_code=True, low_cpu_mem_usage=True, quantization_config=bnb_config_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c17df09-ed2e-4461-860a-b8e43240b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_sequences_generator: TextGenerationPipeline = transformers.pipeline(\n",
    "    task=\"text-generation\",\n",
    "    tokenizer=base_model_tokenizer,\n",
    "    model=base_model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fe3f0bc-74ec-4ccc-b119-7f7afbcd5515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "test_question = \"Find the right word for 'taht'.\\nA: that\\nB: gamma\\nC: parcel\\nD: drawer\"\n",
    "test_prompt: str = prompt_template.format(**{\"question\": test_question})\n",
    "\n",
    "base_model_sequences: list[dict] | list[list[dict]] = base_model_sequences_generator(\n",
    "    text_inputs=test_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=base_model_tokenizer.eos_token_id,\n",
    "    max_length=512,\n",
    "    return_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "033f5b70-fb89-471d-9099-39485d6f1ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GENERATED_TEXT] BASE_MODEL_PREDICTION:\n",
      "\n",
      "Answer this multiple choice question:\n",
      "\n",
      "### Question: Find the right word for 'taht'.\n",
      "A: that\n",
      "B: gamma\n",
      "C: parcel\n",
      "D: drawer\n",
      "\n",
      "### Answer:\n",
      "B: gamma\n",
      "\n",
      "## 7.4\n",
      "\n",
      "Find the right word for 'taht' in the following context\n",
      "Answer this multiple choice question:\n",
      "\n",
      "### Question:\n",
      "``There is a large box full of \\_\\_\\_\\_ in the wardrobe. We'll need it when we hang the curtains.''\n",
      "### Answer:\n",
      "B: stuff\n",
      "\n",
      "## 7.5\n",
      "\n",
      "Here is a dialog in which you take part. Find the appropriate word for the gap:\n",
      "\n",
      "##### A:\n",
      "``Where should I put this book that I just bought?''\n",
      "##### P:\n",
      "``\\_\\_\\_\\_ that in the box in the bookshelf.''\n",
      "\n",
      "### Answer:\n",
      "B: shove\n",
      "\n",
      "## 7.6\n",
      "\n",
      "Fill in the blank with the correct word:\n",
      "\n",
      "### Question:\n",
      "``This is a \\_\\_\\_\\_, the small container to put keys and little things in.''\n",
      "\n",
      "### Answer:\n",
      "A: box\n",
      "\n",
      "## 7.7\n",
      "\n",
      "Listen to the dialog. Where in the dialog does the person ask a question to which he already knows the answer? You can see the dialog in the following transcript:\n",
      "\n",
      "### Dialog\n",
      "```\n",
      "W: There's nothing in that \\_\\_\\_\\_.\n",
      "I: Really? Let me ask you a quick question. Where did you \\_\\_\\_\\_\n",
      "that?''\n",
      "W: You know... It was a total accident. I bought it for my girlfriend\n",
      "at my mom's house one day and then \\_\\_\\_ this thing away while I was\n",
      "at the store.\n",
      "I: Well, it's really a cool \\_\\_\\_, but, I'm not interested.\n",
      "W: Ok ok. I know you didn't want to talk. Just give me my \\_\\_\\_,\n",
      "I'll be gone.\n",
      "```\n",
      "\n",
      "### Answer:\n",
      "I: Really? Let me ask you a quick question. Where did you put\n",
      "that?\n",
      "\n",
      "## 7.8\n",
      "\n",
      "Here is a dialog in which you take part. Find the appropriate word for the gap:\n",
      "\n",
      "##### A:\n",
      "``I just ; TYPE: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "base_model_sequence: dict = base_model_sequences[0]\n",
    "print(f'\\n[GENERATED_TEXT] BASE_MODEL_PREDICTION:\\n{base_model_sequence[\"generated_text\"]} ; TYPE: {str(type(base_model_sequence[\"generated_text\"]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ef21e88-0e7a-461a-aaee-41b4bbdde114",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlora_fine_tuning_config: dict = yaml.safe_load(\n",
    "\"\"\"\n",
    "model_type: llm\n",
    "base_model: alexsherstinsky/Mistral-7B-v0.1-sharded\n",
    "\n",
    "input_features:\n",
    "  - name: question\n",
    "    type: text\n",
    "    preprocessing:\n",
    "      max_sequence_length: 512\n",
    "\n",
    "output_features:\n",
    "  - name: correct_answer\n",
    "    type: text\n",
    "    preprocessing:\n",
    "      max_sequence_length: 128\n",
    "\n",
    "prompt:\n",
    "  template: >-\n",
    "    Answer the following question:\n",
    "\n",
    "    ### Question: {question}\n",
    "\n",
    "    ### Answer:\n",
    "\n",
    "generation:\n",
    "  temperature: 0.1\n",
    "  max_new_tokens: 512\n",
    "\n",
    "adapter:\n",
    "  type: lora\n",
    "#  postprocessor:\n",
    "#    merge_adapter_into_base_model: true\n",
    "#    progressbar: true\n",
    "\n",
    "quantization:\n",
    "  bits: 8\n",
    "\n",
    "preprocessing:\n",
    "  split:\n",
    "    # type: random\n",
    "    # probabilities: [0.9, 0.05, 0.05]\n",
    "    type: fixed\n",
    "\n",
    "trainer:\n",
    "  type: finetune\n",
    "  epochs: 5\n",
    "  batch_size: 1\n",
    "  eval_batch_size: 2\n",
    "  gradient_accumulation_steps: 16  # effective batch size = batch size * gradient_accumulation_steps\n",
    "  learning_rate: 2.0e-4\n",
    "  enable_gradient_checkpointing: true\n",
    "  learning_rate_scheduler:\n",
    "    decay: cosine\n",
    "    warmup_fraction: 0.03\n",
    "    reduce_on_plateau: 0\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "021441ce-ddc8-4895-8a2f-eaacc1c13432",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: LudwigModel = LudwigModel(config=qlora_fine_tuning_config, logging_level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80d04c09-f35c-4828-9376-96da08973629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╒════════════════════════╕\n",
      "│ EXPERIMENT DESCRIPTION │\n",
      "╘════════════════════════╛\n",
      "\n",
      "╒══════════════════╤══════════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Experiment name  │ api_experiment                                                                           │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Model name       │ run                                                                                      │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Output directory │ /workspace/results/api_experiment_run_0                                                  │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ ludwig_version   │ '0.8.6'                                                                                  │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ command          │ ('/workspace/miniconda/envs/llm-env/lib/python3.10/site-packages/ipykernel_launcher.py ' │\n",
      "│                  │  '-f '                                                                                   │\n",
      "│                  │  '/root/.local/share/jupyter/runtime/kernel-276fb877-3df2-4c4c-8b96-a965320f296a.json')  │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ random_seed      │ 42                                                                                       │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ data_format      │ \"<class 'pandas.core.frame.DataFrame'>\"                                                  │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ torch_version    │ '2.1.1+cu121'                                                                            │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ compute          │ {   'arch_list': [   'sm_50',                                                            │\n",
      "│                  │                      'sm_60',                                                            │\n",
      "│                  │                      'sm_70',                                                            │\n",
      "│                  │                      'sm_75',                                                            │\n",
      "│                  │                      'sm_80',                                                            │\n",
      "│                  │                      'sm_86',                                                            │\n",
      "│                  │                      'sm_90'],                                                           │\n",
      "│                  │     'devices': {   0: {   'device_capability': (8, 6),                                   │\n",
      "│                  │                           'device_properties': \"_CudaDeviceProperties(name='NVIDIA \"     │\n",
      "│                  │                                                \"RTX A6000', major=8, minor=6, \"          │\n",
      "│                  │                                                'total_memory=48676MB, '                  │\n",
      "│                  │                                                'multi_processor_count=84)',              │\n",
      "│                  │                           'gpu_type': 'NVIDIA RTX A6000'}},                              │\n",
      "│                  │     'gencode_flags': '-gencode compute=compute_50,code=sm_50 -gencode '                  │\n",
      "│                  │                      'compute=compute_60,code=sm_60 -gencode '                           │\n",
      "│                  │                      'compute=compute_70,code=sm_70 -gencode '                           │\n",
      "│                  │                      'compute=compute_75,code=sm_75 -gencode '                           │\n",
      "│                  │                      'compute=compute_80,code=sm_80 -gencode '                           │\n",
      "│                  │                      'compute=compute_86,code=sm_86 -gencode '                           │\n",
      "│                  │                      'compute=compute_90,code=sm_90',                                    │\n",
      "│                  │     'gpus_per_node': 1,                                                                  │\n",
      "│                  │     'num_nodes': 1}                                                                      │\n",
      "╘══════════════════╧══════════════════════════════════════════════════════════════════════════════════════════╛\n",
      "\n",
      "╒═══════════════╕\n",
      "│ LUDWIG CONFIG │\n",
      "╘═══════════════╛\n",
      "\n",
      "User-specified config (with upgrades):\n",
      "\n",
      "{   'adapter': {'type': 'lora'},\n",
      "    'base_model': 'alexsherstinsky/Mistral-7B-v0.1-sharded',\n",
      "    'generation': {'max_new_tokens': 512, 'temperature': 0.1},\n",
      "    'input_features': [   {   'name': 'question',\n",
      "                              'preprocessing': {'max_sequence_length': 512},\n",
      "                              'type': 'text'}],\n",
      "    'ludwig_version': '0.8.6',\n",
      "    'model_type': 'llm',\n",
      "    'output_features': [   {   'name': 'correct_answer',\n",
      "                               'preprocessing': {'max_sequence_length': 128},\n",
      "                               'type': 'text'}],\n",
      "    'preprocessing': {'split': {'type': 'fixed'}},\n",
      "    'prompt': {   'template': 'Answer the following question:\\n'\n",
      "                              '### Question: {question}\\n'\n",
      "                              '### Answer:'},\n",
      "    'quantization': {'bits': 8},\n",
      "    'trainer': {   'batch_size': 1,\n",
      "                   'enable_gradient_checkpointing': True,\n",
      "                   'epochs': 5,\n",
      "                   'eval_batch_size': 2,\n",
      "                   'gradient_accumulation_steps': 16,\n",
      "                   'learning_rate': 0.0002,\n",
      "                   'learning_rate_scheduler': {   'decay': 'cosine',\n",
      "                                                  'reduce_on_plateau': 0,\n",
      "                                                  'warmup_fraction': 0.03},\n",
      "                   'type': 'finetune'}}\n",
      "\n",
      "Full config saved to:\n",
      "/workspace/results/api_experiment_run_0/api_experiment/model/model_hyperparameters.json\n",
      "\n",
      "╒═══════════════╕\n",
      "│ PREPROCESSING │\n",
      "╘═══════════════╛\n",
      "\n",
      "No cached dataset found at /workspace/c671a168979011ee9ada0242c0a81002.training.hdf5. Preprocessing the dataset.\n",
      "Using full dataframe\n",
      "Building dataset (it may take a while)\n",
      "Loaded HuggingFace implementation of alexsherstinsky/Mistral-7B-v0.1-sharded tokenizer\n",
      "No padding token id found. Using eos_token as pad_token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'None': 56 (without start and stop symbols)\n",
      "Setting max length using dataset: 58 (including start and stop symbols)\n",
      "max sequence length is 58 for feature 'None'\n",
      "Loaded HuggingFace implementation of alexsherstinsky/Mistral-7B-v0.1-sharded tokenizer\n",
      "No padding token id found. Using eos_token as pad_token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'correct_answer': 2 (without start and stop symbols)\n",
      "Setting max length using dataset: 4 (including start and stop symbols)\n",
      "max sequence length is 4 for feature 'correct_answer'\n",
      "Loaded HuggingFace implementation of alexsherstinsky/Mistral-7B-v0.1-sharded tokenizer\n",
      "No padding token id found. Using eos_token as pad_token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of alexsherstinsky/Mistral-7B-v0.1-sharded tokenizer\n",
      "No padding token id found. Using eos_token as pad_token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset: DONE\n",
      "Writing preprocessed training set cache to /workspace/c671a168979011ee9ada0242c0a81002.training.hdf5\n",
      "Writing preprocessed validation set cache to /workspace/c671a168979011ee9ada0242c0a81002.validation.hdf5\n",
      "Writing preprocessed test set cache to /workspace/c671a168979011ee9ada0242c0a81002.test.hdf5\n",
      "Writing train set metadata to /workspace/c671a168979011ee9ada0242c0a81002.meta.json\n",
      "\n",
      "Dataset Statistics\n",
      "╒════════════╤═══════════════╤════════════════════╕\n",
      "│ Dataset    │   Size (Rows) │ Size (In Memory)   │\n",
      "╞════════════╪═══════════════╪════════════════════╡\n",
      "│ Training   │           700 │ 164.19 Kb          │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Validation │           200 │ 47.00 Kb           │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Test       │           100 │ 23.56 Kb           │\n",
      "╘════════════╧═══════════════╧════════════════════╛\n",
      "\n",
      "╒═══════╕\n",
      "│ MODEL │\n",
      "╘═══════╛\n",
      "\n",
      "Warnings and other logs:\n",
      "Loading large language model...\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:13<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Loaded HuggingFace implementation of alexsherstinsky/Mistral-7B-v0.1-sharded tokenizer\n",
      "No padding token id found. Using eos_token as pad_token.\n",
      "==================================================\n",
      "Trainable Parameter Summary For Fine-Tuning\n",
      "Fine-tuning with adapter: lora\n",
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.04703666202518836\n",
      "==================================================\n",
      "Gradient checkpointing enabled for training.\n",
      "\n",
      "╒══════════╕\n",
      "│ TRAINING │\n",
      "╘══════════╛\n",
      "\n",
      "Creating fresh model training run.\n",
      "Training for 3500 step(s), approximately 5 epoch(s).\n",
      "Early stopping policy: 5 round(s) of evaluation, or 3500 step(s), approximately 5 epoch(s).\n",
      "\n",
      "Starting with step 0, epoch: 0\n",
      "Training:   0%|          | 0/3500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/workspace/miniconda/envs/llm-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/workspace/miniconda/envs/llm-env/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 700/3500 [13:11<56:36,  1.21s/it, loss=1.06e-5]   \n",
      "Running evaluation for step: 700, epoch: 0\n",
      "Evaluation valid: 100%|██████████| 100/100 [00:39<00:00,  2.51it/s]\n",
      "Evaluation test : 100%|██████████| 50/50 [00:18<00:00,  2.66it/s]\n",
      "Evaluation took 58.8287s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │    87.3864 │      71.7900 │    71.7800 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     2.1098 │       0.0540 │     0.0003 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 16327.2246 │   11869.0039 │ 11776.5430 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 30998.5508 │   31853.7910 │ 31859.9277 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.1402 │       0.1661 │     0.1664 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.0762 │       0.0910 │     0.0912 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.9318 │       0.9900 │     1.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.1402 │       0.1661 │     0.1664 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.0762 │       0.0910 │     0.0912 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.9318 │       0.9900 │     1.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.1402 │       0.1661 │     0.1664 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.0762 │       0.0910 │     0.0912 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.9318 │       0.9900 │     1.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │    13.4318 │       9.6950 │     9.5400 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     2.1098 │       0.0540 │     0.0003 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Evaluation validation metric: 'correct_answer' 'loss' improved.\n",
      "'correct_answer' 'loss' decreased by inf.\n",
      "New best model saved.\n",
      "\n",
      "Training:  40%|████      | 1400/3500 [27:33<39:46,  1.14s/it, loss=1.57e-5]  \n",
      "Running evaluation for step: 1400, epoch: 1\n",
      "Evaluation valid: 100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n",
      "Evaluation test : 100%|██████████| 50/50 [00:18<00:00,  2.63it/s]\n",
      "Evaluation took 56.4309s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │    80.5455 │      83.8450 │    84.2700 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     0.0006 │       0.0411 │     0.0020 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 11779.7051 │   11851.8799 │ 11795.1914 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 30806.4922 │   30493.1191 │ 30644.4199 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.1592 │       0.1626 │     0.1652 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.0869 │       0.0889 │     0.0903 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     1.0000 │       0.9850 │     1.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.1592 │       0.1626 │     0.1652 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.0869 │       0.0889 │     0.0903 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     1.0000 │       0.9850 │     1.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.1592 │       0.1626 │     0.1652 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.0869 │       0.0889 │     0.0903 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     1.0000 │       0.9850 │     1.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │    11.2955 │      11.8500 │    11.9500 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     0.0006 │       0.0411 │     0.0020 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Evaluation validation metric: 'correct_answer' 'loss' improved.\n",
      "'correct_answer' 'loss' decreased by 0.013.\n",
      "New best model saved.\n",
      "\n",
      "Training:  60%|██████    | 2100/3500 [41:47<26:39,  1.14s/it, loss=4.47e-6]    \n",
      "Running evaluation for step: 2100, epoch: 2\n",
      "Evaluation valid: 100%|██████████| 100/100 [00:36<00:00,  2.71it/s]\n",
      "Evaluation test : 100%|██████████| 50/50 [00:18<00:00,  2.75it/s]\n",
      "Evaluation took 55.2718s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │    83.9318 │      84.6100 │    85.3900 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     0.0001 │       0.0302 │     0.0002 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 11774.4863 │   11816.4424 │ 11774.9629 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 30829.0020 │   30966.3477 │ 30976.8320 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.1723 │       0.1649 │     0.1672 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.0946 │       0.0902 │     0.0916 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     1.0000 │       0.9900 │     1.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.1723 │       0.1649 │     0.1672 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.0946 │       0.0902 │     0.0916 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     1.0000 │       0.9900 │     1.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.1723 │       0.1649 │     0.1672 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.0946 │       0.0902 │     0.0916 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     1.0000 │       0.9900 │     1.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │    11.7045 │      11.9200 │    12.0200 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     0.0001 │       0.0302 │     0.0002 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Evaluation validation metric: 'correct_answer' 'loss' improved.\n",
      "'correct_answer' 'loss' decreased by 0.011.\n",
      "New best model saved.\n",
      "\n",
      "Training:  80%|████████  | 2800/3500 [55:57<13:11,  1.13s/it, loss=1.94e-6]  \n",
      "Running evaluation for step: 2800, epoch: 3\n",
      "Evaluation valid: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
      "Evaluation test : 100%|██████████| 50/50 [00:18<00:00,  2.70it/s]\n",
      "Evaluation took 55.9886s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │    82.7727 │      80.8750 │    81.0700 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     0.0001 │       0.0618 │     0.0379 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 11773.5732 │   11840.0176 │ 11815.9541 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31086.3301 │   31430.7852 │ 31405.1973 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.1714 │       0.1661 │     0.1677 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.0942 │       0.0910 │     0.0920 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     1.0000 │       0.9850 │     0.9900 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.1714 │       0.1661 │     0.1677 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.0942 │       0.0910 │     0.0920 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     1.0000 │       0.9850 │     0.9900 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.1714 │       0.1661 │     0.1677 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.0942 │       0.0910 │     0.0920 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     1.0000 │       0.9850 │     0.9900 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │    11.3182 │      11.3650 │    11.3000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     0.0001 │       0.0618 │     0.0379 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Last improvement of correct_answer validation loss happened 700 step(s) ago.\n",
      "\n",
      "Training: 100%|██████████| 3500/3500 [1:10:05<00:00,  1.13s/it, loss=2.62e-6]\n",
      "Running evaluation for step: 3500, epoch: 4\n",
      "Evaluation valid: 100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\n",
      "Evaluation test : 100%|██████████| 50/50 [00:18<00:00,  2.69it/s]\n",
      "Evaluation took 56.1194s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │    86.0682 │      86.1750 │    88.3500 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     0.0001 │       0.0461 │     0.0116 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 11773.7080 │   11816.0391 │ 11814.5889 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 30869.6914 │   30739.5488 │ 30754.2402 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.1618 │       0.1613 │     0.1613 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.0884 │       0.0881 │     0.0881 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     1.0000 │       0.9900 │     0.9900 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.1618 │       0.1613 │     0.1613 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.0884 │       0.0881 │     0.0881 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     1.0000 │       0.9900 │     0.9900 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.1618 │       0.1613 │     0.1613 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.0884 │       0.0881 │     0.0881 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     1.0000 │       0.9900 │     0.9900 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │    11.8636 │      12.4200 │    12.5900 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     0.0001 │       0.0461 │     0.0116 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Last improvement of correct_answer validation loss happened 1400 step(s) ago.\n",
      "\n",
      "Training: 100%|██████████| 3500/3500 [1:11:02<00:00,  1.22s/it, loss=2.62e-6]\n",
      "\n",
      "╒═════════════════╕\n",
      "│ TRAINING REPORT │\n",
      "╘═════════════════╛\n",
      "\n",
      "╒══════════════════════════════╤════════════════════════╕\n",
      "│ Validation feature           │ correct_answer         │\n",
      "├──────────────────────────────┼────────────────────────┤\n",
      "│ Validation metric            │ loss                   │\n",
      "├──────────────────────────────┼────────────────────────┤\n",
      "│ Best model step              │ 2100                   │\n",
      "├──────────────────────────────┼────────────────────────┤\n",
      "│ Best model epoch             │ 3                      │\n",
      "├──────────────────────────────┼────────────────────────┤\n",
      "│ Best model's validation loss │ 0.0301804319024086     │\n",
      "├──────────────────────────────┼────────────────────────┤\n",
      "│ Best model's test loss       │ 0.00018601860210765153 │\n",
      "╘══════════════════════════════╧════════════════════════╛\n",
      "\n",
      "Finished: api_experiment_run\n",
      "Saved to: /workspace/results/api_experiment_run_0\n",
      "\n",
      "╒══════════╕\n",
      "│ FINISHED │\n",
      "╘══════════╛\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results: TrainingResults = model.train(dataset=df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5df963b5-aa3c-426f-8556-12713949b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: ludwig: command not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ludwig upload hf_hub --repo_id kurtn718/scrambled_multiple_choice --model_path /workspace/results/api_experiment_run_0/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c09e2a08-02a4-4936-8c0a-4f9db924e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors\n",
    "\n",
    "adapter_tensor_dict = safetensors.torch.load_file(\n",
    "    \"./results/api_experiment_run_0/model/model_weights/adapter_model.safetensors\", device=\"cpu\"\n",
    ")\n",
    "torch.save(adapter_tensor_dict, \"./results/api_experiment_run_0/model/model_weights/adapter_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a141619-93dd-4e0b-b1c5-58c0ac6dc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute in terminal\n",
    "#ludwig upload hf_hub --repo_id kurtn718/scrambled_multiple_choice --model_path /workspace/results/api_experiment_run_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072da642-5d53-46d2-bd26-0b2f217e5199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
